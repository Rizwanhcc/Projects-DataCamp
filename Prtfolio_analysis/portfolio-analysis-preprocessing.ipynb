{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f87d33c50d030934a026dedefd83a0b789a57794"
   },
   "source": [
    "## ECE 478 Financial Signal Processing\n",
    "## Pset1: Portoflio Analysis\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "Here we will:\n",
    "1. Take the Farma and French Dataset 48 benchmark, which was acquired from [Kenneth R. French](http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html), clean it, and split its daily returns for the 48 securities into 17 files, one for each year 2000-2016.\n",
    "2. Aquire the S&P500 daily lows and highs from [Yahoo Finance](https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC) and compute the daily returns, organizing the data into years equivalent to the benchmark in step 1.\n",
    "3. Aquire the USD 3 month LIBOR rate from [here](http://iborate.com/usd-libor/). Converte it into an effective daily rate and packaged these daily rates into files by years 2000 - 2016, similar to the steps above. \n",
    "\n",
    "\n",
    "We will begin with reading the Farma and French 48 benchmark dataset, which provides returns for 48 composite portfolios, each of which is made up of stocks from a different sector in the economy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/48_Industry_Portfolios_daily.CSV'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0185b3be2125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdaily_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/48_Industry_Portfolios_daily.CSV\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmonthly_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/48_Industry_Portfolios_Wout_Div.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdaily_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/48_Industry_Portfolios_daily.CSV'"
     ]
    }
   ],
   "source": [
    "daily_data = pd.read_csv(\"../input/48_Industry_Portfolios_daily.CSV\", low_memory=False)\n",
    "monthly_data = pd.read_csv(\"../input/48_Industry_Portfolios_Wout_Div.csv\")\n",
    "\n",
    "daily_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "560becf72470c66e16db479fef30ff041c685e17"
   },
   "source": [
    "We see returns for 48 sectors(Argic, Food, etc.) as well as an unamed first column which gives us the date in a [Year][Month][Day] format.\n",
    "\n",
    "Although we can't see it above, the data  actually appears twice, once for average value weighted returns and again for equally weighted returns. We will be interested in the equally weighted returns data between the years 2000 - 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cb33c4d77e61fd7ce7fa792096b818fe7f72de28"
   },
   "outputs": [],
   "source": [
    "daily_data.rename(columns={'Unnamed: 0': 'Date'}, inplace=True)\n",
    "tmp = daily_data.index[daily_data['Date'] == '19260701'].tolist()\n",
    "equally_weighted_daily_data = daily_data.loc[24267:]\n",
    "\n",
    "equally_weighted_daily_data.set_index('Date', inplace=True) # set the date as the index\n",
    "equally_weighted_daily_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00381463aa3fea143fed22fbaa86e67c0df3107e"
   },
   "source": [
    "We now seperate each year into its own dataset. Note that the year doesn't have to start on Jan 1st and end on Dec 31st, due to the possibility of these dates landing on weekends. In fact, we will say that there are 251 work days in a full year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef6bc20212ad65620fe85673a5832c02a4e1f474"
   },
   "outputs": [],
   "source": [
    "def split_into_years(original_data, beg_year = 2000, amount_years = 17):\n",
    "    list_of_new_datasets = []\n",
    "    for i in range(amount_years):\n",
    "        # due to weekends, need to make sure we start with correct index\n",
    "        start_date = (beg_year+i)*(10**4) + 101\n",
    "        while (str(start_date) not in original_data.index):\n",
    "            start_date += 1\n",
    "\n",
    "        end_date = (beg_year+i)*(10**4) + 1231\n",
    "        while (str(end_date) not in original_data.index):\n",
    "            end_date -= 1\n",
    "\n",
    "        list_of_new_datasets.append(original_data.loc[str(start_date):str(end_date)])\n",
    "    return list_of_new_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9160d6240e643c5ba3cd6d8294089890f6174681"
   },
   "outputs": [],
   "source": [
    "daily_return_by_year = split_into_years(equally_weighted_daily_data) # list of our new datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a04858a4a6274afdd6014e9312b132431cd0134e"
   },
   "source": [
    "We will now deal with the missing data, which the dataset mentions takes the value -99.9 or -999. Just in case, we will search for empty cells also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b35f7a6b4d27dea229ad12e8d8c3769e1aa49bfa"
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(daily_return_by_year):\n",
    "    print('Dataset of:', i+2000, end=' --- ')\n",
    "    for col in data:\n",
    "        count = (data[col] == ' -99.99').sum() + (data[col] == ' -999').sum() + data[col].isnull().sum()\n",
    "        if count > 0:\n",
    "            print(col + ':', count, end=\"; \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94001cb8c2eedf891fcef8aa674792b6537b85b3"
   },
   "source": [
    "Seems that we are okay on the missing data for now. To finish the preprocessing of the benchmark dataset, let us store each new dataset in a different file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "59786d27948b512ce0325914e4d1784287e5152a"
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(daily_return_by_year):\n",
    "    data.to_csv('48_IP_eq_w_daily_returns_' + str(i+2000) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4abd4b62ef844a95c8015195f9135b7d96f1362"
   },
   "source": [
    "We will now move on to the S&P500 returns and USD LIBOR over these same years, making sure to normalize the time scales to daily. Note that we are approximating this approximation by including weekeds in our count, while recognizing that there are no daily returns on those days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eac91a1a0c9e1b322daee6dc7d4590daa8734d42"
   },
   "outputs": [],
   "source": [
    "sp_daily = pd.read_csv(\"../input/dailySP500.CSV\")\n",
    "sp_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c425aa630bc72b8d4c04f6f122c600a2f6fb5ab2"
   },
   "outputs": [],
   "source": [
    "print('Values to impute in Open:', sp_daily['Open'].isnull().sum())\n",
    "print('Values to impute in Adj Close:', sp_daily['Adj Close'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9535141466667c85d32d47f6a0b01af1734f3e4"
   },
   "source": [
    "Our data is already clean, so let us move on to finding the daily returns with the formula:\n",
    "$$ Daily \\ \\ Return = \\frac{(Adj\\ \\ Close) - (Open)}{Open} $$\n",
    "and then proceed to split up these returns into individual years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76b5f142c0f3809e3e365c7759913d24feedcc12"
   },
   "outputs": [],
   "source": [
    "temp_sp_ID =  [ i.replace('-', '') for i in sp_daily['Date'] ]\n",
    "temp_sp_returns = 100 * (sp_daily['Adj Close'] - sp_daily['Open'] ) / sp_daily['Open']\n",
    "\n",
    "sp_returns = pd.DataFrame({'Date':  temp_sp_ID, 'Return': temp_sp_returns })\n",
    "sp_returns.set_index('Date', inplace=True)\n",
    "\n",
    "sp_returns_by_year = split_into_years(sp_returns)\n",
    "\n",
    "for i, data in enumerate(sp_returns_by_year):\n",
    "    data.to_csv('SP_daily_returns_' + str(i+2000) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "651a1aabf079cf1a91478f04c2f27aeefd104be7"
   },
   "source": [
    "For the USD LIBOR, we will first format the date identically to our benchmark and S&P500 returns and then we will convert the 3 month dollar LIBOR into an effective daily rate using the equation:\n",
    "$$ R =  100 \\big[ (1+ \\frac{R_m}{100})^{1/N} - 1 \\big] $$\n",
    "\n",
    "where $R$ is the daily interest rate, $m$  is the amount of times daily interest will be compounded per 3 month period, and $R_m$ is the 3 month interest rate.\n",
    "\n",
    "Assuming a 260 day work year, we can say that there there are $\\frac{251}{4} = 62$ work days per 3-month period, and so in our case $ m = 62$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "81b990cc1fbafe17034dd398e0d9eaa6214edda9"
   },
   "outputs": [],
   "source": [
    "libor_daily = pd.read_csv(\"../input/LIBOR USD.csv\")\n",
    "libor_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "439c9ed7ccd90b83d6745b7d00f257a8dffd6a15"
   },
   "outputs": [],
   "source": [
    "temp_id = []\n",
    "for i in libor_daily['Date']:\n",
    "    temp = i.split('.')\n",
    "    temp_id.append(temp[2] + temp[1] + temp[0])\n",
    "\n",
    "libor_daily['Date'] = temp_id\n",
    "libor_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5fad3ab07b9151933c95642728068574d138ec6c"
   },
   "outputs": [],
   "source": [
    "print('Values to impute in 3M column:', libor_daily['3M'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "746b37bcbe20938033cbf18e34a9c5ae18c352e6"
   },
   "outputs": [],
   "source": [
    "m = 62\n",
    "\n",
    "temp_daily_rate = [100*( (1 + i/100)**(1/m) -1) for i in libor_daily['3M'] ]\n",
    "\n",
    "libor_intr = pd.DataFrame({ 'Date': libor_daily['Date'], 'Effective Daily Interest': temp_daily_rate })\n",
    "libor_intr.set_index('Date', inplace=True)\n",
    "\n",
    "libor_intr_by_year = split_into_years(libor_intr)\n",
    "\n",
    "for i, data in enumerate(libor_intr_by_year):\n",
    "    data.to_csv('LIBOR_daily_interest_' + str(i+2000) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3f51aaa15895cadcfa4c20848718ee0adaefd7e7"
   },
   "source": [
    "and thats it! The actual analysis of this data is conducted [here](https://www.kaggle.com/guybaryosef/portfolio-assesment)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
